<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Nazia Tasnim</title> <meta name="author" content="Nazia Tasnim"> <meta name="description" content=""> <meta name="keywords" content="graduate, intern, machine-learning, AI, PEFT, NLP, CV, research, scientist, PhD, blogs, anime, academic-website, portfolio-website"> <meta property="og:site_name" content="Nazia Tasnim"> <meta property="og:type" content="website"> <meta property="og:title" content="Nazia Tasnim | START"> <meta property="og:url" content="https://appledora.github.io/"> <meta property="og:description" content=""> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="START"> <meta name="twitter:description" content=""> <meta name="twitter:site" content="@nimzia_nothere"> <meta name="twitter:creator" content="@nimzia_nothere"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/mask.webp?22ef6fcf739231e2d2323a95392ae037"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://appledora.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script>!function(){"use strict";function t(){const t=document.getElementById("light-toggle");t&&!t.dataset.setup&&(t.dataset.setup="true",t.addEventListener("click",function(t){t.preventDefault();const e=document.documentElement,n="dark"===(e.getAttribute("data-theme")||"light")?"light":"dark";e.classList.add("transition"),e.setAttribute("data-theme",n),localStorage.setItem("theme",n),setTimeout(()=>e.classList.remove("transition"),750)}))}function e(){t(),!document.getElementById("light-toggle")&&o<d&&(o++,setTimeout(e,50))}const n=localStorage.getItem("theme");n&&document.documentElement.setAttribute("data-theme",n);let o=0;const d=10;if(e(),"loading"===document.readyState&&(document.addEventListener("DOMContentLoaded",t),document.addEventListener("readystatechange",function(){"interactive"===document.readyState&&t()})),"undefined"!=typeof MutationObserver){const e=new MutationObserver(function(){t()});document.documentElement&&e.observe(document.documentElement,{childList:!0,subtree:!0})}}();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6E%69%6D%7A%69%61@%62%75.%65%64%75" title="email"><i class="fa-solid fa-envelope fa-xs"></i></a> <a href="https://scholar.google.com/citations?user=_9JYPbsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar fa-xs"></i></a> <a href="https://github.com/appledora" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github fa-xs"></i></a> <a href="https://www.linkedin.com/in/nazianim-blu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin fa-xs"></i></a> <a href="https://twitter.com/nimzia_nothere" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter fa-xs"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">START<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">EXTRA</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Nazia</span> Tasnim </h1> <p class="desc"><a href="https://www.bu.edu/cs/ivc/people/" rel="external nofollow noopener" target="_blank">IVC-ML </a>, Boston University, Boston, MA.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/nazia_dp-480.webp 480w, /assets/img/nazia_dp-800.webp 800w, /assets/img/nazia_dp-1400.webp 1400w, " sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/nazia_dp.jpg?e647d69c283489399c9638297fb5bf39" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="nazia_dp.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> Here's me being Pochita at the Newbury Comics! <br> 📮 nimzia [at] bu [dot] edu </div> </div> <div class="clearfix"> <p>Hello there!! I am a CS PhD student at Boston University, advised by Prof. <a href="https://bryanplummer.com/" rel="external nofollow noopener" target="_blank">Bryan Plummer</a> on topics related to <strong>multimodal learning</strong> and <strong>parameter-efficient finetuning</strong>. My research interest lies at the crosssection of <strong>Natural Language Processing</strong> and <strong>Computer Vision</strong>, with a special emphasis on <strong>responsible applications</strong> and <strong>under-resourced domains</strong>.</p> <ul> <li>I was fortunate to be advised by <a href="https://isaacjoh.com/" rel="external nofollow noopener" target="_blank">Dr. Isaac Johnson</a> and <a href="https://martingerlach.github.io/about/" rel="external nofollow noopener" target="_blank">Dr. Martin Gerlach</a> at the <strong>Wikimedia Research Team</strong>, where I built resources and pipelines for NLP research across the Wikimedia projects.</li> <li>I have <strong>published</strong> in multiple A* conferences and workshops, including Interspeech, NeuRIPS, EMNLP, and CVPR. A full list of my publications can be found <a href="/publications">here</a>.</li> <li>I have a year of <strong>industry experience</strong> as a Machine Learning Engineer at <a href="https://gigatechltd.com/" rel="external nofollow noopener" target="_blank">Giga Tech Ltd.</a>, where I worked on developing the <em>National Syntactic TreeBank for Bangla.</em> I am also a Research Collaborator at <a href="https://bengaliai.github.io/" rel="external nofollow noopener" target="_blank">Bengali.ai</a>, where I work on building resources for low-resource languages.</li> <li>I also build different <strong>open-source tools</strong> and <strong>datasets</strong> to support the larger community. Some of my projects can be found <a href="/projects">here</a>.</li> </ul> <p>Outside research and courseworks, I am interested in plants🪴, pottery 🏺 and pop-culture 🖖. Shoot me an email if you want to collaborate on a project, have a question, or just want to say hi~ </p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <table> <tr class="news-item"> <td class="news-date"> Sep 22, 2025 </td> <td class="news-content"> Joined as an Instructor for the NSF REU program at Boston University </td> </tr> <tr class="news-item"> <td class="news-date"> Jun 22, 2025 </td> <td class="news-content"> Attended CVPR'25 as a WiCV mentee in Nashville, TN </td> </tr> <tr class="news-item"> <td class="news-date"> May 22, 2025 </td> <td class="news-content"> Second PhD paper is online! Read it on : <a href="https://arxiv.org/abs/2505.21649" target="_blank" rel="noopener">arXiv</a> 📄 </td> </tr> <tr class="news-item"> <td class="news-date"> Apr 24, 2025 </td> <td class="news-content"> Presented my paper at ICLR'25 in Singapore 🇸🇬 </td> </tr> <tr class="news-item"> <td class="news-date"> Apr 06, 2025 </td> <td class="news-content"> Attended the CRA-W Grad Cohort 2025 in Denver, CO </td> </tr> </table> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">vlm</abbr> </div> <div class="venue-badge">under review</div> </div> <img src="/assets/img/publication_preview/dori.png" alt="Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks" class="preview" loading="lazy"> <div class="title">Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks</div> <div class="author"> </div> <div class="abstract-content"> <p>Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene understanding. We introduce DORI (Discriminative Orientation Reasoning Intelligence), a comprehensive benchmark establishing object orientation perception as a primary evaluation target. DORI assesses four dimensions of orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. Through carefully curated tasks from 11 datasets spanning 67 object categories across synthetic and real-world scenarios, DORI provides insights on how multi-modal systems understand object orientations. Our evaluation of 15 state-of-the-art vision-language models reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the need for dedicated orientation representation mechanisms, as models show systematic inability to perform precise angular estimations, track orientation changes across viewpoints, and understand compound rotations - suggesting limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for orientation awareness in multimodal systems, DORI offers implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments.</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/abs/2505.21649" class="btn" target="_blank">PDF</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">PEFT</abbr> </div> <div class="venue-badge">ICLR</div> </div> <img src="/assets/img/publication_preview/recast.png" alt="RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks" class="preview" loading="lazy"> <div class="title">RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks</div> <div class="author"> </div> <div class="abstract-content"> <p>Incremental learning aims to adapt to new sets of categories over time with minimal computational overhead. Prior work often addresses this task by training efficient task-specific adaptors that modify frozen layer weights or features to capture relevant information without affecting predictions on previously learned categories. While these adaptors are generally more efficient than finetuning the entire network, they still require tens to hundreds of thousands of task-specific trainable parameters even for relatively small networks, making it challenging to operate on resource-constrained environments with high communication costs like edge devices or mobile phones. Thus, we propose Reparameterized, Compact weight Adaptation for Sequential Tasks (RECAST), a novel method that dramatically reduces task-specific trainable parameters to fewer than 50 - several orders of magnitude less than competing methods like LoRA. RECAST accomplishes this efficiency by learning to decompose layer weights into a soft parameter-sharing framework consisting of shared weight templates and very few module-specific scaling factors or coefficients. This soft parameter-sharing framework allows for effective task-wise reparameterization by tuning only these coefficients while keeping templates frozen.A key innovation of RECAST is the novel weight reconstruction pipeline called Neural Mimicry, which eliminates the need for pretraining from scratch. This allows for high-fidelity emulation of existing pretrained weights within our framework and provides quick adaptability to any model scale and architecture</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/abs/2411.16870" class="btn" target="_blank">PDF</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">vision</abbr> </div> <div class="venue-badge">NeuRIPS</div> </div> <img src="/assets/img/publication_preview/vlm.png" alt="Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks" class="preview" loading="lazy"> <div class="title">Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks</div> <div class="author"> </div> <div class="abstract-content"> <p>Typographic attacks, adding misleading text to images, can deceive vision-language models (LVLMs). The susceptibility of recent large LVLMs like GPT4-V to such attacks is understudied, raising concerns about amplified misinformation in personal assistant applications. Previous attacks use simple strategies, such as random misleading words, which don’t fully exploit LVLMs’ language reasoning abilities. We introduce an experimental setup for testing typographic attacks on LVLMs and propose two novel self-generated attacks: (1) Class-based attacks, where the model identifies a similar class to deceive itself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack combining a deceiving class and description. Our experiments show these attacks significantly reduce classification performance by up to 60% and are effective across different models, including InstructBLIP and MiniGPT4. Code: https://github.com/mqraitem/Self-Gen-Typo-Attack</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/pdf/2402.00626" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.48550/arXiv.2402.00626" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">speech</abbr> </div> <div class="venue-badge">INTERSPEECH</div> </div> <img src="/assets/img/publication_preview/ood.png" alt="OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking" class="preview" loading="lazy"> <div class="title">OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking</div> <div class="author"> </div> <div class="abstract-content"> <p>We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Being one of the most spoken languages globally, Bengali portrays large diversity in dialects and prosodic features, which demands ASR frameworks to be robust towards distribution shifts. For example, islamic religious sermons in Bengali are delivered with a tonality that is significantly different from regular speech. Our training dataset is collected via massively online crowdsourcing campaigns which resulted in 1177.94 hours collected and curated from native Bengali speakers from South Asia. Our test dataset comprises 23.03 hours of speech collected and manually annotated from 17 different sources, e.g., Bengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to name a few. OOD-Speech is jointly the largest publicly available speech dataset, as well as the first out-of-distribution ASR benchmarking dataset for Bengali.</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/pdf/2305.09688" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.48550/arXiv.2305.09688" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">vision</abbr> </div> <div class="venue-badge">CVPR</div> </div> <img src="/assets/img/publication_preview/vista.png" alt="Vista: vision transformer enhanced by u-net and image colorfulness frame filtration for automatic retail checkout" class="preview" loading="lazy"> <div class="title">Vista: vision transformer enhanced by u-net and image colorfulness frame filtration for automatic retail checkout</div> <div class="author"> </div> <div class="abstract-content"> <p>Multi-class product counting and recognition identifies product items from images or videos for automated retail checkout. The task is challenging due to the real-world scenario of occlusions where product items overlap, fast movement in conveyor belt, large similarity in overall appearance of the items being scanned, novel products, the negative impact of misidentifying items. Further there is a domain bias between training and test sets, specifically the provided training dataset consists of synthetic images and the test set videos consist of foreign objects such as hands and tray. To address these aforementioned issues, we propose to segment and classify individual frames from a video sequence. The segmentation method consists of a unified single product item-and hand-segmentation followed by entropy masking to address the domain bias problem. The multi-class classification method is based on Vision Transformers (ViT). To identify the frames with target objects, we utilize several image processing methods and propose a custom metric to discard frames not having any product items. Combining all these mechanisms, our best system achieves 3rd place in the AI City Challenge 2022 Track 4 with F1 score of 0.4545.</p> </div> <div class="links"> <a href="/assets/pdf/https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Shihab_VISTA_Vision_Transformer_Enhanced_by_U-Net_and_Image_Colorfulness_Frame_CVPRW_2022_paper.pdf" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.1109/cvprw56347.2022.00359" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">nlp</abbr> </div> <div class="venue-badge">SemEval</div> </div> <img src="/assets/img/publication_preview/multi.png" alt="On leveraging data augmentation and ensemble to recognize complex named entities in bangla" class="preview" loading="lazy"> <div class="title">On leveraging data augmentation and ensemble to recognize complex named entities in bangla</div> <div class="author"> </div> <div class="abstract-content"> <p>Many areas, such as the biological and healthcare domain, artistic works, and organization names, have nested, overlapping, discontinuous entity mentions that may even be syntactically or semantically ambiguous in practice. Traditional sequence tagging algorithms are unable to recognize these complex mentions because they may violate the assumptions upon which sequence tagging schemes are founded. In this paper, we describe our contribution to SemEval 2022 Task 11 on identifying such complex Named Entities. We have leveraged the ensemble of multiple ELECTRA-based models that were exclusively pretrained on the Bangla language with the performance of ELECTRA-based models pretrained on English to achieve competitive performance on the Track-11. Besides providing a system description, we will also present the outcomes of our experiments on architectural decisions, dataset augmentations, and post-competition findings.</p> </div> <div class="links"> <a href="/assets/pdf/https://aclanthology.org/2022.semeval-1.209.pdf" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.18653/v1/2022.semeval-1.209" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">bio</abbr> </div> <div class="venue-badge">Briefings in Bioinformatics</div> </div> <img src="/assets/img/publication_preview/bob.jpeg" alt="Choice of assemblers has a critical impact on de novo assembly of sars-cov-2 genome and characterizing variants" class="preview" loading="lazy"> <div class="title">Choice of assemblers has a critical impact on de novo assembly of sars-cov-2 genome and characterizing variants</div> <div class="author"> </div> <div class="links"> <a href="/assets/pdf/https://academic.oup.com/bib/article/22/5/bbab102/6210065?login=false" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.1093/bib/bbab102" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © This website has been modified (rather terribly) from the OG al-folio theme.Last updated: October 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script src="/assets/js/retro-8bit.js"></script> <script src="/assets/js/publications-expand.js" defer></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>