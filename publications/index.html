<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Nazia Tasnim</title> <meta name="author" content="Nazia Tasnim"> <meta name="description" content=""> <meta name="keywords" content="graduate, intern, machine-learning, AI, PEFT, NLP, CV, research, scientist, PhD, blogs, anime, academic-website, portfolio-website"> <meta property="og:site_name" content="Nazia Tasnim"> <meta property="og:type" content="website"> <meta property="og:title" content="Nazia Tasnim | publications"> <meta property="og:url" content="https://appledora.github.io/publications/"> <meta property="og:description" content=""> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content=""> <meta name="twitter:site" content="@nimzia_nothere"> <meta name="twitter:creator" content="@nimzia_nothere"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/mask.webp?22ef6fcf739231e2d2323a95392ae037"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://appledora.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script>!function(){"use strict";function t(){const t=document.getElementById("light-toggle");t&&!t.dataset.setup&&(t.dataset.setup="true",t.addEventListener("click",function(t){t.preventDefault();const e=document.documentElement,n="dark"===(e.getAttribute("data-theme")||"light")?"light":"dark";e.classList.add("transition"),e.setAttribute("data-theme",n),localStorage.setItem("theme",n),setTimeout(()=>e.classList.remove("transition"),750)}))}function e(){t(),!document.getElementById("light-toggle")&&o<d&&(o++,setTimeout(e,50))}const n=localStorage.getItem("theme");n&&document.documentElement.setAttribute("data-theme",n);let o=0;const d=10;if(e(),"loading"===document.readyState&&(document.addEventListener("DOMContentLoaded",t),document.addEventListener("readystatechange",function(){"interactive"===document.readyState&&t()})),"undefined"!=typeof MutationObserver){const e=new MutationObserver(function(){t()});document.documentElement&&e.observe(document.documentElement,{childList:!0,subtree:!0})}}();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Nazia </span>Tasnim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">START</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/prize/">accolades</a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">EXTRA</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> <div class="tools"> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">vlm</abbr> </div> <div class="venue-badge">under review</div> </div> <img src="/assets/img/publication_preview/dori.png" alt="Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks" class="preview" loading="lazy"> <div class="title">Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks</div> <div class="author"> </div> <div class="abstract-content"> <p>Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene understanding. We introduce DORI (Discriminative Orientation Reasoning Intelligence), a comprehensive benchmark establishing object orientation perception as a primary evaluation target. DORI assesses four dimensions of orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. Through carefully curated tasks from 11 datasets spanning 67 object categories across synthetic and real-world scenarios, DORI provides insights on how multi-modal systems understand object orientations. Our evaluation of 15 state-of-the-art vision-language models reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the need for dedicated orientation representation mechanisms, as models show systematic inability to perform precise angular estimations, track orientation changes across viewpoints, and understand compound rotations - suggesting limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for orientation awareness in multimodal systems, DORI offers implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments.</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/abs/2505.21649" class="btn" target="_blank">PDF</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">PEFT</abbr> </div> <div class="venue-badge">ICLR</div> </div> <img src="/assets/img/publication_preview/recast.png" alt="RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks" class="preview" loading="lazy"> <div class="title">RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks</div> <div class="author"> </div> <div class="abstract-content"> <p>Incremental learning aims to adapt to new sets of categories over time with minimal computational overhead. Prior work often addresses this task by training efficient task-specific adaptors that modify frozen layer weights or features to capture relevant information without affecting predictions on previously learned categories. While these adaptors are generally more efficient than finetuning the entire network, they still require tens to hundreds of thousands of task-specific trainable parameters even for relatively small networks, making it challenging to operate on resource-constrained environments with high communication costs like edge devices or mobile phones. Thus, we propose Reparameterized, Compact weight Adaptation for Sequential Tasks (RECAST), a novel method that dramatically reduces task-specific trainable parameters to fewer than 50 - several orders of magnitude less than competing methods like LoRA. RECAST accomplishes this efficiency by learning to decompose layer weights into a soft parameter-sharing framework consisting of shared weight templates and very few module-specific scaling factors or coefficients. This soft parameter-sharing framework allows for effective task-wise reparameterization by tuning only these coefficients while keeping templates frozen.A key innovation of RECAST is the novel weight reconstruction pipeline called Neural Mimicry, which eliminates the need for pretraining from scratch. This allows for high-fidelity emulation of existing pretrained weights within our framework and provides quick adaptability to any model scale and architecture</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/abs/2411.16870" class="btn" target="_blank">PDF</a> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">vision</abbr> </div> <div class="venue-badge">NeuRIPS</div> </div> <img src="/assets/img/publication_preview/vlm.png" alt="Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks" class="preview" loading="lazy"> <div class="title">Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks</div> <div class="author"> </div> <div class="abstract-content"> <p>Typographic attacks, adding misleading text to images, can deceive vision-language models (LVLMs). The susceptibility of recent large LVLMs like GPT4-V to such attacks is understudied, raising concerns about amplified misinformation in personal assistant applications. Previous attacks use simple strategies, such as random misleading words, which don’t fully exploit LVLMs’ language reasoning abilities. We introduce an experimental setup for testing typographic attacks on LVLMs and propose two novel self-generated attacks: (1) Class-based attacks, where the model identifies a similar class to deceive itself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack combining a deceiving class and description. Our experiments show these attacks significantly reduce classification performance by up to 60% and are effective across different models, including InstructBLIP and MiniGPT4. Code: https://github.com/mqraitem/Self-Gen-Typo-Attack</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/pdf/2402.00626" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.48550/arXiv.2402.00626" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">speech</abbr> </div> <div class="venue-badge">INTERSPEECH</div> </div> <img src="/assets/img/publication_preview/ood.png" alt="OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking" class="preview" loading="lazy"> <div class="title">OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking</div> <div class="author"> </div> <div class="abstract-content"> <p>We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Being one of the most spoken languages globally, Bengali portrays large diversity in dialects and prosodic features, which demands ASR frameworks to be robust towards distribution shifts. For example, islamic religious sermons in Bengali are delivered with a tonality that is significantly different from regular speech. Our training dataset is collected via massively online crowdsourcing campaigns which resulted in 1177.94 hours collected and curated from native Bengali speakers from South Asia. Our test dataset comprises 23.03 hours of speech collected and manually annotated from 17 different sources, e.g., Bengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to name a few. OOD-Speech is jointly the largest publicly available speech dataset, as well as the first out-of-distribution ASR benchmarking dataset for Bengali.</p> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/pdf/2305.09688" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.48550/arXiv.2305.09688" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">vision</abbr> </div> <div class="venue-badge">CVPR</div> </div> <img src="/assets/img/publication_preview/vista.png" alt="Vista: vision transformer enhanced by u-net and image colorfulness frame filtration for automatic retail checkout" class="preview" loading="lazy"> <div class="title">Vista: vision transformer enhanced by u-net and image colorfulness frame filtration for automatic retail checkout</div> <div class="author"> </div> <div class="abstract-content"> <p>Multi-class product counting and recognition identifies product items from images or videos for automated retail checkout. The task is challenging due to the real-world scenario of occlusions where product items overlap, fast movement in conveyor belt, large similarity in overall appearance of the items being scanned, novel products, the negative impact of misidentifying items. Further there is a domain bias between training and test sets, specifically the provided training dataset consists of synthetic images and the test set videos consist of foreign objects such as hands and tray. To address these aforementioned issues, we propose to segment and classify individual frames from a video sequence. The segmentation method consists of a unified single product item-and hand-segmentation followed by entropy masking to address the domain bias problem. The multi-class classification method is based on Vision Transformers (ViT). To identify the frames with target objects, we utilize several image processing methods and propose a custom metric to discard frames not having any product items. Combining all these mechanisms, our best system achieves 3rd place in the AI City Challenge 2022 Track 4 with F1 score of 0.4545.</p> </div> <div class="links"> <a href="/assets/pdf/https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Shihab_VISTA_Vision_Transformer_Enhanced_by_U-Net_and_Image_Colorfulness_Frame_CVPRW_2022_paper.pdf" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.1109/cvprw56347.2022.00359" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">nlp</abbr> </div> <div class="venue-badge">SemEval</div> </div> <img src="/assets/img/publication_preview/multi.png" alt="On leveraging data augmentation and ensemble to recognize complex named entities in bangla" class="preview" loading="lazy"> <div class="title">On leveraging data augmentation and ensemble to recognize complex named entities in bangla</div> <div class="author"> </div> <div class="abstract-content"> <p>Many areas, such as the biological and healthcare domain, artistic works, and organization names, have nested, overlapping, discontinuous entity mentions that may even be syntactically or semantically ambiguous in practice. Traditional sequence tagging algorithms are unable to recognize these complex mentions because they may violate the assumptions upon which sequence tagging schemes are founded. In this paper, we describe our contribution to SemEval 2022 Task 11 on identifying such complex Named Entities. We have leveraged the ensemble of multiple ELECTRA-based models that were exclusively pretrained on the Bangla language with the performance of ELECTRA-based models pretrained on English to achieve competitive performance on the Track-11. Besides providing a system description, we will also present the outcomes of our experiments on architectural decisions, dataset augmentations, and post-competition findings.</p> </div> <div class="links"> <a href="/assets/pdf/https://aclanthology.org/2022.semeval-1.209.pdf" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.18653/v1/2022.semeval-1.209" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">nlp</abbr> </div> <div class="venue-badge">KDD-Health</div> </div> <img src="/assets/img/publication_preview/kdd.png" alt=" Exploring the Scope and Potential of Local Newspaper-based Dengue Surveillance in Bangladesh" class="preview" loading="lazy"> <div class="title"> Exploring the Scope and Potential of Local Newspaper-based Dengue Surveillance in Bangladesh</div> <div class="author"> </div> <div class="links"> <a href="/assets/pdf/https://arxiv.org/pdf/2107.14095" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.48550/arXiv.2107.14095" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">nlp</abbr> </div> <div class="venue-badge">FLAIRS</div> </div> <img src="/assets/img/publication_preview/dengue.png" alt="Observing the unobserved: a newspaper based dengue surveillance system for the low-income regions of bangladesh" class="preview" loading="lazy"> <div class="title">Observing the unobserved: a newspaper based dengue surveillance system for the low-income regions of bangladesh</div> <div class="author"> </div> <div class="abstract-content"> <p>Dengue is one of the emerging diseases of this century, which established itself as both endemic and epidemic-particularly in the tropical and subtropical-regions. Because of its high morbidity and mortality rates, Dengue is a significant economic and health burden for middle to lower-income countries. The lack of a stable, cost-effective, and suitable surveillance system has made the identification of dengue zones and designing potential control programs very challenging. As a result, it is not feasible to assess the effect of the intervention actions properly. Therefore, most of the prevention and mitigation efforts by the associated health officials are failing. In this work, we chose Bangladesh, a developing country from the South-East Asia region with its occasional history of dengue outbreaks and with a high out-of-pocket medical expenditure, as a use case. We use some well known data-mining techniques on the local newspapers, written in Bengali, to unearth valuable insights and develop a dengue news surveillance system. We categorize dengue-news and detect the spatio-temporal trends among crucial variables. Our technique provides an f-score of 91.45% and very closely follows the ground truth of reported cases. Additionally, we identify the under-reported regions of the country effectively while establishing a meaningful relationship between complex socio-economic factors and reporting of dengue.</p> </div> <div class="links"> <a href="/assets/pdf/https://journals.flvc.org/FLAIRS/article/view/128552" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.32473/flairs.v34i1.128552" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> <li> <li class="publication-item"> <div class="publication-header"> <div class="abbr"> <abbr class="badge">bio</abbr> </div> <div class="venue-badge">Briefings in Bioinformatics</div> </div> <img src="/assets/img/publication_preview/bob.jpeg" alt="Choice of assemblers has a critical impact on de novo assembly of sars-cov-2 genome and characterizing variants" class="preview" loading="lazy"> <div class="title">Choice of assemblers has a critical impact on de novo assembly of sars-cov-2 genome and characterizing variants</div> <div class="author"> </div> <div class="links"> <a href="/assets/pdf/https://academic.oup.com/bib/article/22/5/bbab102/6210065?login=false" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.1093/bib/bbab102" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <li class="publication-item"> <div class="publication-header"> </div> <img src="/assets/img/publication_preview/vehicle.png" alt="Team Passphrase: Dhaka AI Vehicle Detection Challenge 4th place Submission" class="preview" loading="lazy"> <div class="title">Team Passphrase: Dhaka AI Vehicle Detection Challenge 4th place Submission</div> <div class="author"> </div> <div class="abstract-content"> <p>Traffic detection comes with many different challenges depending on the specific characteristics of the problem domain. Dhaka traffic has its own distinctive elements and dynamics. To design an effective object detection model that captures the essence of dhaka traffic, careful model optimization and data engineering is needed. So, we focus on developing a robust pipeline using an ensemble of existing SOTA models, to ensure improved generalization and less dependency on model complexity.</p> </div> <div class="links"> <a href="/assets/pdf/https://www.researchgate.net/profile/Sowmen-Das/publication/356039571_Team_Passphrase_Dhaka_AI_Vehicle_Detection_Challenge_4th_place_Submission/links/618a431b07be5f31b75c5c7f/Team-Passphrase-Dhaka-AI-Vehicle-Detection-Challenge-4th-place-Submission.pdf" class="btn" target="_blank">PDF</a> <a href="https://doi.org/10.13140/RG.2.2.18174.72006" class="btn" target="_blank" rel="external nofollow noopener">DOI</a> </div> </li> </ol> </div> </article> </div> </header> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © This website has been modified (rather terribly) from the OG al-folio theme.Last updated: October 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script src="/assets/js/retro-8bit.js"></script> <script src="/assets/js/publications-expand.js" defer></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </div> </body> </html>